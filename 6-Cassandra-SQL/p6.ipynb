{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375264a8-d589-4951-bebe-89c251c870e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carson Batchelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccb8764-3399-410b-b23f-b355688ae191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.29.0.3  104.33 KiB  16      69.7%             b138ace0-45d7-4b47-9efe-dde10abc7bbc  rack1\n",
      "UN  172.29.0.2  104.33 KiB  16      66.3%             afdb2570-0c26-48ce-b534-59dbd35dd143  rack1\n",
      "UN  172.29.0.4  104.32 KiB  16      63.9%             e8659979-0ce6-4c7e-a02e-3beaaf4352d2  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45742ee-846d-48b9-9612-bcc8bde3ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8847234b-c12c-4ebc-80b0-989e83204148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc8c832e170>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP keyspace IF EXISTS weather\")\n",
    "cass.execute(\"\"\"\n",
    "    CREATE keyspace weather\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}\n",
    "\"\"\")\n",
    "cass.execute(\"USE weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b173e2aa-6973-44e5-8bc9-760a773ffb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc8c832eef0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP table IF EXISTS station\")\n",
    "cass.execute(\"DROP type IF EXISTS station_record\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TYPE station_record (\n",
    "        tmin int,\n",
    "        tmax int)\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TABLE stations (\n",
    "        id text,\n",
    "        name text STATIC,\n",
    "        date date,\n",
    "        record station_record,\n",
    "        PRIMARY KEY (id, date)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92b1cd8-fa4d-47b8-b385-7fac32ba1fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d7d578-36d3-47e7-beaa-3ef752682243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d01b20ac-e7dc-4786-95af-fed77244e548;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (198ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (111ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (272ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (169ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (102ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (40ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (44ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (38ms)\n",
      ":: resolution report :: resolve 5568ms :: artifacts dl 1385ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d01b20ac-e7dc-4786-95af-fed77244e548\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/83ms)\n",
      "23/11/15 01:06:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9115fd7-97f4-486b-88c5-02874b6633e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"ghcnd-stations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9038501-af07-4326-b8d5-5a23c2a66147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036fb2ab-59f6-4d2f-b302-bdbf35dbe5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string, station_ID: string, state: string, station_name: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.withColumn(\"station_ID\", expr(\"substring(value, 0, 11)\"))\n",
    "df2 = df2.withColumn(\"state\", expr(\"substring(value, 39, 2)\"))\n",
    "df2 = df2.withColumn(\"station_name\", expr(\"substring(value, 42, 30)\"))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45be39e4-a934-4350-a1af-b8fb257bcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wisconsin = df2.filter(df2.state == \"WI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9912b40a-03ab-469f-8c79-4bc0b2cbcd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wi_stations = df_wisconsin.collect()\n",
    "for row in wi_stations:\n",
    "    station_id = row[\"station_ID\"].strip()\n",
    "    station_name = row[\"station_name\"].strip()\n",
    "    # Assuming you have a function to execute the INSERT statement, replace the following line accordingly\n",
    "    cass.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO weather.stations (id, name)\n",
    "        VALUES (%s, %s)\n",
    "        \"\"\",\n",
    "        (station_id, station_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02617e0-599c-4ed8-a42f-c815152aed2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   1313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(cass.execute(\"SELECT COUNT(*) FROM weather.stations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146a16a3-af3b-4a0d-81a0-d2b6d8e438e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(cass.execute(\"SELECT * FROM weather.stations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca10658-474f-4725-b633-d0f7b3bb0e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "pd.DataFrame(cass.execute(\"\"\"\n",
    "    SELECT name \n",
    "    FROM weather.stations \n",
    "    WHERE id = 'USW00014837'\n",
    "    \"\"\"))[\"name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f42250f-8f54-41c0-ba0d-2957a83c5568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "token = pd.DataFrame(cass.execute(\"\"\"\n",
    "    SELECT token(id) \n",
    "    FROM weather.stations \n",
    "    WHERE id = 'USC00470273'\n",
    "    \"\"\"))[\"system_token_id\"][0]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f005a848-c2e8-4bdd-9aa5-e3021c41c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "nodetool_output = subprocess.check_output([\"nodetool\", \"ring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d5fdc6-0763-4b64-889e-cdaf7c0d57ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = nodetool_output.decode(\"utf-8\").splitlines()\n",
    "token_list = []\n",
    "for line in lines:\n",
    "    start = 80\n",
    "    end = line.find(' ', start)\n",
    "    token_list.append(line[80:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30784549-344a-40de-9ef3-b46376ee63a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8946087620239790516"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "target_token = token\n",
    "for token in token_list[5:53]:\n",
    "    if int(token) >= int(target_token):\n",
    "        smallest_token = str(token)\n",
    "        break\n",
    "    else:\n",
    "        smallest_token = token_list[0]\n",
    "int(smallest_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553dea0f-0a58-448c-b35b-5d61d31cd345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m grpc_tools.protoc -I=. --python_out=. --grpc_python_out=. station.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419cee37-dedc-42e6-9af7-6afabaf949a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"records.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c3b920b-ab5d-415f-856a-6398706acd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_filter = df.filter((col(\"element\").isin([\"TMIN\", \"TMAX\"])))\n",
    "df_grouped = df_filter.groupBy(\"station\", \"date\").pivot(\"element\").agg(expr(\"first(value)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1788e546-9d88-4377-a6d2-2fdb439435c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc, station_pb2, station_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "client = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "rows = df_grouped.collect()\n",
    "\n",
    "for row in rows:\n",
    "    year = row[\"date\"][0:4]\n",
    "    month = row[\"date\"][4:6]\n",
    "    day = row[\"date\"][6:8]\n",
    "    \n",
    "    station_id = row[\"station\"]\n",
    "    date = str(f\"{year}-{month}-{day}\")\n",
    "    tmin = int(row[\"TMIN\"])\n",
    "    tmax = int(row[\"TMAX\"])\n",
    "\n",
    "    response = client.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "        station=station_id,\n",
    "        date=date,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "732db2da-0fcb-4bba-8e15-f829c480cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 01:17:27 WARN ChannelPool: [s0|p6-db-2/172.29.0.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=a7a64868-12e6-4683-9374-88ab5f30ce2b, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700010370528}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q5\n",
    "id = 'USW00014837'\n",
    "response = client.StationMax(station_pb2.StationMaxRequest(station=id))\n",
    "response.tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0be6883-961f-4626-8978-1fc4d1d4b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "    .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    "    .option(\"keyspace\", \"weather\")\n",
    "    .option(\"table\", \"stations\")\n",
    "    .load())\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dbdc3de-1508-486a-bf01-80461a13adfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f4880a-c5f1-44c2-afd7-bed5a4d6c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USW00014837': 105.62739726027397,\n",
       " 'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014898': 102.93698630136986,\n",
       " 'USW00014839': 89.6986301369863}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "from pyspark.sql.functions import avg, col\n",
    "results = spark.sql(\"\"\"\n",
    "    SELECT id, AVG(record.tmax - record.tmin) AS avg_dif\n",
    "    FROM stations\n",
    "    WHERE record IS NOT NULL\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "dict(results.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f183754d-a8c6-4b7e-aad0-b4c5e52ee108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "DN  172.29.0.3  86.02 KiB  16      100.0%            b138ace0-45d7-4b47-9efe-dde10abc7bbc  rack1\n",
      "UN  172.29.0.2  86.01 KiB  16      100.0%            afdb2570-0c26-48ce-b534-59dbd35dd143  rack1\n",
      "UN  172.29.0.4  86.01 KiB  16      100.0%            e8659979-0ce6-4c7e-a02e-3beaaf4352d2  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "091504a2-f97c-4d46-a792-7635472c6be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: \"need 3 replicas, but only have 2\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "client = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "id = 'USW00014839'\n",
    "response = client.StationMax(station_pb2.StationMaxRequest(station=id))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbe5feb9-48b0-4bfd-8cfa-bd45c3f7681d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 01:18:27 WARN ChannelPool: [s0|p6-db-2/172.29.0.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=a7a64868-12e6-4683-9374-88ab5f30ce2b, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700010370528}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "station_id = 'USW00014837'\n",
    "date = \"2022-03-07\"\n",
    "tmin = 25\n",
    "tmax = 130\n",
    "\n",
    "response = client.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "    station=station_id,\n",
    "    date=date,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a4340-61da-4ff3-8622-19b1b11953a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5aebcb-a05a-43fa-8732-21886c803ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
